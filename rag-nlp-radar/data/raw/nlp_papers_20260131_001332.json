[
  {
    "id": "http://arxiv.org/abs/2601.22159v1",
    "arxiv_id": "2601.22159v1",
    "title": "RedSage: A Cybersecurity Generalist LLM",
    "abstract": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
    "authors": [
      "Naufal Suryanto",
      "Muzammal Naseer",
      "Pengfei Li",
      "Syed Talal Wasim",
      "Jinhui Yi",
      "Juergen Gall",
      "Paolo Ceravolo",
      "Ernesto Damiani"
    ],
    "published": "2026-01-29T18:59:57+00:00",
    "updated": "2026-01-29T18:59:57+00:00",
    "pdf_url": "https://arxiv.org/pdf/2601.22159v1",
    "primary_category": "cs.CR",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22157v1",
    "arxiv_id": "2601.22157v1",
    "title": "Discovering Hidden Gems in Model Repositories",
    "abstract": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.",
    "authors": [
      "Jonathan Kahana",
      "Eliahu Horwitz",
      "Yedid Hoshen"
    ],
    "published": "2026-01-29T18:59:55+00:00",
    "updated": "2026-01-29T18:59:55+00:00",
    "pdf_url": "https://arxiv.org/pdf/2601.22157v1",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "id": "http://arxiv.org/abs/2601.22156v1",
    "arxiv_id": "2601.22156v1",
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "abstract": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
    "authors": [
      "Yingfa Chen",
      "Zhen Leng Thai",
      "Zihan Zhou",
      "Zhu Zhang",
      "Xingyu Shen",
      "Shuo Wang",
      "Chaojun Xiao",
      "Xu Han",
      "Zhiyuan Liu"
    ],
    "published": "2026-01-29T18:59:53+00:00",
    "updated": "2026-01-29T18:59:53+00:00",
    "pdf_url": "https://arxiv.org/pdf/2601.22156v1",
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  }
]